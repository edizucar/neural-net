{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-27T23:31:49.976031Z","iopub.execute_input":"2023-11-27T23:31:49.976456Z","iopub.status.idle":"2023-11-27T23:31:49.983905Z","shell.execute_reply.started":"2023-11-27T23:31:49.976426Z","shell.execute_reply":"2023-11-27T23:31:49.982626Z"},"trusted":true},"execution_count":295,"outputs":[]},{"cell_type":"markdown","source":"# Make some activation functions for the hell of it","metadata":{}},{"cell_type":"code","source":"# Lets get some activation functions going:\n\nsoftmax = lambda x : np.exp(x) / np.sum(np.exp(x), axis=0)\n\nsigmoid = lambda x : 1/(1+np.exp(-x))\n\nsigmoid_derivative = lambda x : sigmoid(x) * (1 - sigmoid(x))\n\nrelu = lambda x : np.maximum(0,x)\n\nidentity = lambda x : x\n\none = lambda x : 1","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:49.986007Z","iopub.execute_input":"2023-11-27T23:31:49.986363Z","iopub.status.idle":"2023-11-27T23:31:50.002904Z","shell.execute_reply.started":"2023-11-27T23:31:49.986334Z","shell.execute_reply":"2023-11-27T23:31:50.001817Z"},"trusted":true},"execution_count":296,"outputs":[]},{"cell_type":"markdown","source":"# Defining a layer","metadata":{}},{"cell_type":"markdown","source":"Note: these were an experiment and not actually used","metadata":{}},{"cell_type":"code","source":"\nclass Layer():\n    def __init__(self, width, activation=identity):\n        self.width = width\n        self.activation = activation\n        self.weights = None\n        self.bias = None\n        self.input_width = None\n        \n    def construct(self, input_width):\n        # input_width is the width of the previous layer\n        if self.input_width != None:\n            raise AttributeError(\"Already initialised\")\n        else:\n            self.input_width = input_width\n            self.weights = np.random.rand(self.width, self.input_width)\n            self.bias = np.random.rand()\n            \n    def eval(self,input_values):\n        if self.input_width == None:\n            raise AttributeError(\"Not initialised\")\n        else:\n            return self.activation((self.weights @ input_values) + self.bias)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.005293Z","iopub.execute_input":"2023-11-27T23:31:50.005680Z","iopub.status.idle":"2023-11-27T23:31:50.017075Z","shell.execute_reply.started":"2023-11-27T23:31:50.005634Z","shell.execute_reply":"2023-11-27T23:31:50.015813Z"},"trusted":true},"execution_count":297,"outputs":[]},{"cell_type":"markdown","source":"# Defining a Neural Network in terms of layers","metadata":{}},{"cell_type":"code","source":"square_cost_derivative = lambda x,y : x - y\nsquare_cost = lambda x,y : (x - y) * (x - y) / 2\n\nclass NeuralNetwork:\n    def __init__(self, sizes, act=sigmoid, act_deriv=sigmoid_derivative, cost=square_cost, cost_derivative=square_cost_derivative):\n        \"\"\"\n        sizes:list of the sizes of layers\n        \"\"\"\n        self.n_layers = len(sizes)\n        self.sizes = sizes\n        \n        # Don't need biases for inputs so we do sizes[1:]\n        self.biases = [np.random.randn(size) for size in sizes[1:]]\n        \n        # Need weights between the layers.\n        # n x m matrix where n is # of perceptrons on output layer\n        # and m is # of perceptrons on input layer\n        self.weights = [np.random.randn(n_out,n_in) for n_in,n_out in zip(sizes[:-1],sizes[1:])]\n        \n        self.activation = act\n        self.activation_derivative = act_deriv\n        self.cost = cost\n        self.cost_derivative = cost_derivative\n        \n    def ff(self, input_values):\n        for weight, bias in zip(self.weights,self.biases):\n            print(\"weights: \", weight.shape, \" biases: \", bias.shape, \" input: \", input_values.shape)\n            input_values = self.activation(weight @ input_values + bias)\n            print(input_values)\n        return input_values\n    \n    def SGD(self, training_set, epochs=1, mini_batch_size=1, learning_rate=1):\n        for i in range(epochs):\n            mini_batches = self.split_training_set(training_set,mini_batch_size)\n            \n            for mb in mini_batches:\n                self.update_mini_batch(mb, learning_rate)\n                \n    def update_mini_batch(self, mini_batch, learning_rate):\n        change_biases = [np.zeros(bias.shape) for bias in self.biases]\n        change_weights = [np.zeros(weight.shape) for weight in self.weights]\n        \n        for x,y in mini_batch:\n            delta_biases, delta_weights = self.backprop(x,y)\n            change_biases = [old + d for old,d in zip(change_biases, delta_biases)]\n            change_weights = [old + d for old,d in zip(change_weights, delta_weights)]\n        self.weights = [w + (learning_rate/len(mini_batch)) * cw for w,cw in zip(self.weights,change_weights)]\n        self.biases = [b + (learning_rate/len(mini_batch)) * cb for b,cb in zip(self.biases,change_biases)]\n        \n    def backprop(self,x,y):\n        # x is vector for the inputs\n        # y is also a vector but for the outputs\n        \n        activation = x\n        activations = [activation]\n        nets = []\n        \n        # feed forward and store activations\n        \n        for w,b in zip(self.weights,self.biases):\n            net = w @ activation + b\n            nets.append(net)\n            \n            activation = self.activation(net)\n            activations.append(activation)\n            \n        # backwards pass\n        delta = self.cost_derivative(activations[-1],y) * self.activation_derivative(activation)\n        \n        change_b = [np.zeros(b.shape) for b in self.biases]\n        change_w = [np.zeros(w.shape) for w in self.weights]\n        \n        #print(\"delta:\",delta,delta.reshape(-1,1).shape)\n        #print(\"activations:\",activations[-2].shape)\n    \n                        \n        change_b[-1] = delta\n        change_w[-1] = delta.reshape(-1,1) @ activations[-2].reshape(1,-1)\n        \n        for i in range(2,self.n_layers):\n            net = nets[-i]\n            delta = (self.weights[-i + 1].T @ delta) * self.activation_derivative(net)\n            change_b[-i] = delta\n            change_w[-i] = delta.reshape(-1,1) @ activations[-i-1].reshape(1,-1)\n            \n        return change_b,change_w\n    \n    def split_training_set(self,training_set,mini_batch_size):\n        np.random.shuffle(training_set)\n        return [training_set[k:k+mini_batch_size] for k in range(0, len(training_set), mini_batch_size)]\n            \n            \n            \n    \n    def __str__(self):\n        s = []\n        s.append(\" \".join([f\"input({n+1})\" for n in range(self.sizes[0])]))\n        for weight,bias in zip(self.weights,self.biases):\n            s.append(\"   |\")\n            s.append(\"   \\\\/\")\n            s.append(np.array2string(weight) + \"  +  \" + np.array2string(bias))\n        return \"\\n\".join(s)\n            \n        ","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.113428Z","iopub.execute_input":"2023-11-27T23:31:50.113817Z","iopub.status.idle":"2023-11-27T23:31:50.144925Z","shell.execute_reply.started":"2023-11-27T23:31:50.113787Z","shell.execute_reply":"2023-11-27T23:31:50.143904Z"},"trusted":true},"execution_count":298,"outputs":[]},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"nn = NeuralNetwork([2,3,2])\nprint(nn)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.146781Z","iopub.execute_input":"2023-11-27T23:31:50.148385Z","iopub.status.idle":"2023-11-27T23:31:50.165924Z","shell.execute_reply.started":"2023-11-27T23:31:50.148345Z","shell.execute_reply":"2023-11-27T23:31:50.164214Z"},"trusted":true},"execution_count":299,"outputs":[{"name":"stdout","text":"input(1) input(2)\n   |\n   \\/\n[[ 0.55081434 -0.97159026]\n [-0.68799406 -0.60930652]\n [-1.39479904 -0.90646672]]  +  [ 1.67108451 -1.1276874  -1.47927503]\n   |\n   \\/\n[[ 0.58200173  0.2985557  -0.55920355]\n [ 2.65583507  1.17920684 -1.72684983]]  +  [0.44407812 1.4606961 ]\n","output_type":"stream"}]},{"cell_type":"code","source":"nn.ff(np.array([0,0]))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.168080Z","iopub.execute_input":"2023-11-27T23:31:50.168493Z","iopub.status.idle":"2023-11-27T23:31:50.180955Z","shell.execute_reply.started":"2023-11-27T23:31:50.168461Z","shell.execute_reply":"2023-11-27T23:31:50.179894Z"},"trusted":true},"execution_count":300,"outputs":[{"name":"stdout","text":"weights:  (3, 2)  biases:  (3,)  input:  (2,)\n[0.84172036 0.24458813 0.18553695]\nweights:  (2, 3)  biases:  (2,)  input:  (3,)\n[0.71161304 0.97501536]\n","output_type":"stream"},{"execution_count":300,"output_type":"execute_result","data":{"text/plain":"array([0.71161304, 0.97501536])"},"metadata":{}}]},{"cell_type":"code","source":"training_set = [(np.array([1,1]),np.array([1,1])),\n               (np.array([1,3]),np.array([1,3])),\n               (np.array([2,1]),np.array([2,1])),\n               (np.array([0,1]),np.array([0,1])),]","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.182916Z","iopub.execute_input":"2023-11-27T23:31:50.184282Z","iopub.status.idle":"2023-11-27T23:31:50.194089Z","shell.execute_reply.started":"2023-11-27T23:31:50.184233Z","shell.execute_reply":"2023-11-27T23:31:50.192586Z"},"trusted":true},"execution_count":301,"outputs":[]},{"cell_type":"code","source":"nn.SGD(training_set,epochs=100)","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.197517Z","iopub.execute_input":"2023-11-27T23:31:50.198522Z","iopub.status.idle":"2023-11-27T23:31:50.263373Z","shell.execute_reply.started":"2023-11-27T23:31:50.198472Z","shell.execute_reply":"2023-11-27T23:31:50.262066Z"},"trusted":true},"execution_count":302,"outputs":[]},{"cell_type":"code","source":"nn.ff(np.array([1,5]))","metadata":{"execution":{"iopub.status.busy":"2023-11-27T23:31:50.265345Z","iopub.execute_input":"2023-11-27T23:31:50.265821Z","iopub.status.idle":"2023-11-27T23:31:50.276568Z","shell.execute_reply.started":"2023-11-27T23:31:50.265777Z","shell.execute_reply":"2023-11-27T23:31:50.274974Z"},"trusted":true},"execution_count":303,"outputs":[{"name":"stdout","text":"weights:  (3, 2)  biases:  (3,)  input:  (2,)\n[1.34647474e-09 2.24228227e-04 1.00000000e+00]\nweights:  (2, 3)  biases:  (2,)  input:  (3,)\n[1.26479463e-082 1.57287965e-123]\n","output_type":"stream"},{"execution_count":303,"output_type":"execute_result","data":{"text/plain":"array([1.26479463e-082, 1.57287965e-123])"},"metadata":{}}]}]}
